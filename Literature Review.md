---
title: Available methods for finding security vulnerabilities in software
subtitle: Literature Review for ENGR201
author:
    - Dylan Chong
date: \today{}
abstract: |
    TODO?

    ab

    str
---

-------------------------------------------------------------------------------

# Introduction

Security vulnerabilities in software can lead to security breaches by hackers,
which usually lead to data loss or corruption, denial of service issues and
privacy issues [1]. Breaches cost billions of dollars every year [5]. A study
of data security breaches showed that issues can reduce customer trust in the
affected company / organisation, while positively affecting competitors [3] -
this allows for the extortion of companies / organisations. The number of
security issues, especially for financial software, has increased [2], raising
concerns about customer finances.

Effective security testing techniques can be used to detect vulnerabilities in
software, therefore reducing the probability of a breach. Effective techniques
are those that find a large number of vulnerabilities, with as few false
vulnerabilities as possible. They should be time and resource-efficent.

This review will be useful for security testing researchers, developers, as
well as executives who work for any organisation involved with software. This
review also provides areas that potentially need to be researched in this area.

# Definitions

This section outlines some existing security testing methods that have been
developed or used in testing methods described in the 'Available Security
Testing Methods' section.

## Fuzz Testing

Fuzz testing is a testing approach where different program inputs are
automatically executed against the Program Under Test, and if an
exception / error occurs, a potential security vulnerability is revealed.
Inputs are often automatically altered in order to generate new program inputs
for further testing [5]. The inputs for a single execution of the program make
up a single test case. All of the program inputs (i.e. all of the test cases)
make up a test suite.

Black-Box Fuzz testing methods do not require the source code to be able to
generate program inputs. White-Box Fuzz testing methods, by definition, require
access to the program source code. Symbolic Execution can be used in White-Box
Fuzz testing methods - it involves inspecting the code to figure out the inputs
required to execute all possible execution paths [5].

# Available Security Testing Methods

Much of the research into security testing in the last five years appears to
have focussed around the development of automated security testing methods.
SimFuzz and the STRIDE approach focus on selecting/generating high-risk test
cases, and static code analysis tools automatically detect vulnerabilities in
source code.

## SimFuzz

Zhang et al ([5]) proposed a new approach for generating and selecting test
cases for deep execution paths, which vulnerabilities are likely to exit. It as
been implemented in a tool called SimFuzz for experimentation. The proposed
approach uses a Test Case Similarity metric (TCS) in order to find similar
execution paths to some original deep path, in order to provide high coverage
for the deep states [5].

The SimFuzz approach can be broken down into two phases. The first phase
involves Incremental Mutation: breaking up a valid input into segments and
making small alterations to each (each new input makes a new test case). The
second phase involves combining multiple test cases from the first phase in
order to create additional test case inputs. Both of these phases create inputs
that have high TCS, ensuring high coverage of the involved states [5].

Executing generated SimFuzz tests is very efficient for large programs because
test cases are created intentionally and selectively - the existing White-Box
testing approach using Symbolic Execution creates a large number of test cases.
In comparison with the existing Fuzz testing approach, the SimFuzz approach has
a much higher coverage of deep execution paths because most test cases
generated by fuzzing only test shallow execution paths [5].

## STRIDE Risk Analysis and Threat Modelling

Palanivel and Selvadurai [4] proposed a security testing process that adds the
STRIDE threat modelling approach used by Microsoft to an existing risk analysis
approach in order to select the highest priority tests from a generated test
suite. This reduces the amount of time required to execute the test suite. The
test suite can be generated by techniques such as Black or White-Box Fuzz Test
Generation [4].

It can be broken down into four phases: State Representation, Threat Modelling,
Risk Analysis, Test Case Selection. The State Representation phase involves the
identification of system states and transitions. The Threat Modelling phase
involves using knowledge of the states plus a data flow diagram, in combination
with the STRIDE model to identify what security threats are possible in each
state. The results from this phase are used in the Risk Analysis phase, where a
risk value is calculated for each threat. Finally, the tests that involve a
high number of high risk states are selected from a generated set of test cases
[4].

The proposed process was successfully able to reduce the generated test suite
size by 13-21% in the example cases [4], i.e. 13-21% low risk test cases were
removed. This shows that the proposed process was able to identify the high
risk test cases.

## Static Code Analysis Tools

Static code analysis tools inspect the source code of a software project to
detect a predefined range of vulnerabilities. A study compared several multiple
C code analysis tools by using them on software with known vulnerabilities.
Most of the tools, individually, were only designed to detect a limited range
of vulnerabilities. The vulnerability detection rate was too low for any of the
tools to be used as a primary method for security testing - at best, the
detection rate was 67%. These tools are easy to execute, however, results can
show false vulnerabilities, so the results of the analysis must be checked by a
developer [6].

# Discussion

The SimFuzz approach could be combined with Palanivel and Kanmani's risk
analysis and threat modelling approach in order to prioritise the testing of
execution paths that involve high risk operations. Prioritised paths can then
be tested more thoroughly in order to increase the probability of finding
hidden vulnerabilities. Work is required to test the feasibility of this
approach.

Zhang et al intend to extend the SimFuzz tool to work with binary executables
and web applications [5]. The application of the tool to the web could be
particularly useful especially in a language such as JavaScript which uses weak
typing - a feature that can cause unexpected issues to arise. Potentially it
may be possible to simulate concurrent systems which are hard to test
thoroughly and prone to unpredictable behaviour at runtime - therefore it may
be worth testing these systems for vulnerabilities.

Vulnerabilities were intentionally introduced to programs tested by SimFuzz
[5], so the promising test results do not represent real programs. More work is
needed to test SimFuzz on real-world programs in order to prove its
effectiveness.

One possible disadvantage of the STRIDE approach is that it may take a
significant amount of time for the State Representation phase. Large software
projects would have an extremely large number of states, so it would be
unfeasible to complete this phase manually. Automation of this process is
likely to be the only feasible large-scale option, however the paper did not
consider this problem. Automated solutions must be explored, otherwise the
entire STRIDE approach is impractical.

Static code analysis tools tested by Diaz and Bermejo [6] only test for a
limited range of vulnerabilities, therefore it is not recommended to heavily
rely on these tools. Despite this, it is still highly recommended to use these
tools because they are easy to execute and they were still able to detect
37-67% of the vulnerabilities in the tested code [6]. More work is needed
identifying what static code analysis tools are effective for programming
languages other than C, as well as extending tools to detect a larger range of
vulnerabilities.

# Conclusion

Much of the research into security testing in the last five years shows
promise. The SimFuzz approach was able to thoroughly test the example software
[5], could be extended to support concurrent systems, and could be combined
with the STRIDE approach to thoroughly test high-risk parts of software. More
work is needed to determine the viability of the STRIDE approach, and
investigate/extend static code analysis tools.

Such security testing methods help to discover vulnerabilities in software
systems. Automated testing processes can provide significant assistance to the
testing process. They could be more effective at security testing than humans
at throroughly testing large software systems, due to their potential speed.
Developments in these security testing methods could help to prevent privacy
leaks, data loss, and loss of billions of dollars \[1][5].

# References

[1] A. Khan. (2014). "QUALITY SOFTWARE & SECURITY TESTING AND ITS IMPACT ON
SOFTWARE PRODUCT," International Journal of Technology and Research. [Online].
Vol. 2 (4), pp. 114-117. Available:
https://search-proquest-com.helicon.vuw.ac.nz/docview/1721566667?accountid=14782
[Aug. 20, 2017].

[2] M. Bland. "Finding more than one worm in the apple". Communications of the
ACM". [Online]. Vol. 57 (7), pp. 58-64. 2014. Available:
http://dl.acm.org.helicon.vuw.ac.nz/citation.cfm?doid=2622628.2622630 [Aug. 20,
2017].

[3] P. McCole el al. (2010, Sept - Oct). "Trust considerations on attitudes
towards online purchasing: The moderating effect of privacy and security
concerns". Journal of Business Research. [Online]. Vol. 63 (9-10), pp.
1018-1024. Available:
http://www.sciencedirect.com.helicon.vuw.ac.nz/science/article/pii/
S0148296309001933?via%3Dihub [Aug. 20, 2017].

[4] M. Palanivel, K. Selvadurai. (2014). "Risk-driven security testing using
risk analysis with threat modeling approach". SpringerPlus, [Online]. Vol. 3
(1), pp. 754. Available:
https://springerplus.springeropen.com/articles/10.1186/2193-1801-3-754 [Aug.
20, 2017].

[5] D. Zhang et al. (2012, Jan). "SimFuzz: Test case similarity directed deep
fuzzing," Journal of Systems and Software. [Online]. Vol. 85 (1), pp. 102-111.
Available: http://www.sciencedirect.com.helicon.vuw.ac.nz/science/article/pii/
S016412121100197X?via%3Dihub [Aug. 20, 2017].

[6] G. DÃ­az, J. R. Bermejo. (2013, Aug). "Static analysis of source code
security: Assessment of tools against SAMATE tests". Information and Software
Technology. [Online]. Vol. 55 (8), pp. 1462-1476. Available:
http://www.sciencedirect.com/science/article/pii/S0950584913000384?via%3Dihub
[Aug. 20, 2017].
